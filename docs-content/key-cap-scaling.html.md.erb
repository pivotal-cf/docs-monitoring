---
title: Key Capacity Scaling Indicators
owner: PCF Metrics Platform Monitoring
---

This topic describes key capacity scaling indicators that operators monitor to determine when
they need to scale their Pivotal Cloud Foundry (PCF) deployments.

Pivotal provides these indicators to operators as general guidance for capacity scaling. 
Each indicator is based on platform metrics from different components. 
This guidance is applicable to most PCF v1.10 deployments.
Pivotal recommends that operators fine-tune the suggested alert thresholds 
by observing historical trends for their deployments. 

##<a id="cell"></a>Diego Cell Capacity Scaling Indicators

Currently, there are three key capacity scaling indicators recommended for Diego cell:

+ [Diego Cell Memory Capacity](#cell-memory) is a measure of the percentage of remaining memory capacity
+ [Diego Cell Disk Capacity](#cell-disk) is a measure of the percentage of remaining disk capacity
+ [Diego Cell Container Capacity](#cell-container) is a measure of the percentage of remaining container capacity

###<a id="cell-memory"></a>Diego Cell Memory Capacity

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory / rep.CapacityTotalMemory<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
     <td> Percentage of remaining memory capacity for a given cell. Should be looked at holistically across all cells in a deployment.
<br><br>
      Derived by dividing `rep.CapacityRemainingMemory` (remaining amount in MiB of memory available for this cell to allocate to containers) by `rep.CapacityTotalMemory` (total amount in MiB of memory available for this cell to allocate to containers).</td>
 </tr>
   <tr>
      <th>Purpose</th>
      <td>Best practice deployment of Cloud Foundry recommends 3 Availability zones.  
          For these types of deployments it is suggested to have enough capacity to suffer failure of a complete availability Zone.
          <br><br>
          Recommended threshold is based on this 3 Availability Zone configuration. The threshold percentage should be altered if more or less Availability Zones. 
      </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&lt; avg(30%)</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>      
      Scale up your Diego Cells 
      </td>
   </tr>
    <tr>
      <th>Additional Details</th>
      <td> <strong>Origin</strong>: Doppler/Firehose<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:diego_cells
      </td>
   </tr>
</table>

###<a id="cell-disk"></a>Diego Cell Disk Capacity

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingDisk / rep.CapacityTotalDisk<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Percentage of remaining disk capacity for a given cell. Should be looked at holistically across all cells in a deployment.
<br><br>
      Derived by dividing `rep.CapacityRemainingDisk` (remaining amount in MiB of disk available for this cell to allocate to containers) by `rep.CapacityTotalDisk`(total amount in MiB of disk available for this cell to allocate to containers).</td>
 </tr>
   <tr>
       <th>Purpose</th>
      <td>Best practice deployment of Cloud Foundry recommends 3 Availability zones.  
          For these types of deployments it is suggested to have enough capacity to suffer failure of a complete availability Zone.
          <br><br>
          Recommended threshold is based on this 3 Availability Zone configuration. The threshold percentage should be altered if more or less Availability Zones. 
      </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&lt; avg(30%)</td>
   </tr>
   <tr>
      <th>How to Scale</th>
      <td>      
      Scale up your Diego Cells 
      </td>
   </tr>
    <tr>
      <th>Additional Details</th>
      <td> <strong>Origin</strong>: Doppler/Firehose<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:diego_cells
      </td>
   </tr>
</table>

###<a id="cell-container"></a>Diego Cell Container Capacity

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingContainers / 
       rep.CapacityTotalContainers<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Percentage of remaining container capacity for a given cell. Should be looked at holistically across all cells in a deployment.
      <br><br>
      Derived by dividing `rep.CapacityRemainingContainers` (remaining number of containers this cell can host) by `rep.CapacityTotalContainer`(total number of containers this cell can host).</td>
   <tr>
      <th>Purpose</th>
      <td>Best practice deployment of Cloud Foundry recommends 3 Availability zones.  
          For these types of deployments it is suggested to have enough capacity to suffer failure of a complete availability Zone.
          <br><br>
          Recommended threshold is based on this 3 Availability Zone configuration. The threshold percentage should be altered if more or less Availability Zones. 
      </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&lt; avg(30%)</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>
      Scale up your Diego Cells 
     </td>
   </tr>
    <tr>
      <th>Additional Details</th>
      <td> <strong>Origin</strong>: Doppler/Firehose<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:diego_cells
      </td>
   </tr>
</table>

## <a id="doppler-server"></a> Firehose Performance Scaling Indicator
Currently, there is one key capacity scaling indicator recommended for Firehose performance. 

### <a id="firehose-loss-rate"></a> Firehose Loss Rate

<table>
  <tr>
      <th colspan="2" style="text-align: center;">((DopplerServer.TruncatingBuffer.totalDroppedMessages + 
          DopplerServer.doppler.shedEnvelopes) / DopplerServer.listeners.totalReceivedMessageCount)</th>
  </tr>
   <tr>
      <th width="25">Description</th>
      <td> This derived value represents the firehose loss rate, or the total messages dropped as a % of the total message throughput
       </td>
   </tr>
   <tr>
      <th>Purpose</th>
      <td>Excessive dropped messages can indicate the Dopplers are not processing messages fast enough 
        <br><br>
        Recommended scaling indicator is to look at the total dropped as a % of the total throughput. Then scale if this derived loss rate value grows greater than 0.1
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 0.1</br>
      If alerting:<br>
      <strong>Yellow warning</strong>: &ge; 0.05<br>
      <strong>Red critical</strong>: &ge; 0.1</td>
   </tr>
   <tr>
      <th>How to scale up</th>
      <td>Scale up the Firehose log receiver and Dopplers. 
      </td>
   </tr>
   <tr>
      <th>Additional Details</th>
      <td> <strong>Origin</strong>: Doppler/Firehose<br>
           <strong>Type</strong>: Gauge (float)<br>
           <strong>Frequency</strong>: Base metrics are emitted every 5 s<br>
           <strong>Applies to</strong>: cf:doppler<br>
      </td>
   </tr>
</table>

(Amber) We have a known issue with the Dropped Messages metric that is impacting the ability to publish this derived value suggestion. Working with Loggregator team to resolve

## <a id="Router"></a> Router Performance Scaling Indicator
Currently, there is one key capacity scaling indicator recommended for Router performance. 

###<a id="system.cpu.user"></a>Router VM CPU Utilization

<table>
  <tr>
      <th colspan="2" style="text-align: center;">system.cpu.user of Gorouter VM(s)</th>
  </tr>
  <tr>
      <th>Description</th>
      <td>CPU utilization of the Gorouter VM(s)</td>
  </tr>
  <tr>
      <th>Purpose</th>
      <td>High CPU utilization of the Gorouter VMs can cause an increase in latency, as well as a leveling-off of throughput (requests per/second). Pivotal recommends keeping the CPU utilization within a maximum 60-70% range for best Gorouter performance. 
      <br><br>
      For operators wishing to increase throughput capabilities while also keeping latency low, it is recommended to scale the Gorouter while continuing to ensure the maximum recommended CPU utilization range is not exceeded. 
          </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 60%<br>
      If alerting:<br>
      <strong>Yellow warning</strong>: &ge; 60%<br>
      <strong>Red critical</strong>: &ge; 70%<br>
      </td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>Resolve high utilization by scaling the Gorouters horizontally or vertically (the <b>Router</b> VM in the <b>Resource Config</b> pane of the Elastic Runtime tile). 
      </td>
   </tr>
   <tr>
      <th>Additional Details</th>
      <td> <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
           <strong>Type</strong>: Gauge (float)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:router<br>
      </td>
   </tr>
</table>
          
Amber: What does it mean for throughput "to level off" ?

##<a id="systemdiskpersist"></a>NFS/WebDAV Backed Blobstore

<p class="note"><strong>Note</strong>: Relevant when not leveraging an external S3 repository for external storage with no capacity constraints</p>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>system.disk.persistent.percent of NFS server VM(s)<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td><em>If applicable</em>: Monitor the percentage of persistent disk used on the VM for the NFS Server job.
           </tr>
   <tr>
       <th>Purpose</th>
       <td>When not leveraging an external S3 repository for external storage with no capacity constraints, the object store for Cloud Foundry must be monitored to push new applications and buildpacks.
     <br><br>
     When leveraging an internal NFS/WebDAV backed blobstore, consider scaling the persistent disk when 75% capacity is reached.
     </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&ge; 75%</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>
      Give your NFS Server additional persistent disk resources.  
      </td>
   </tr>
    <tr>
      <th>Additional Details</th>
      <td> <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Applies to</strong>: cf:nfs_server<br>
      </td>
   </tr>  
</table>
